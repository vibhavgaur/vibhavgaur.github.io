<!DOCTYPE html>
<html lang="en">

<head>
	<title>Least-Squares&nbsp;fitting</title>
	<link rel="stylesheet" type="text/css" href="/theme/css/base.css"/>
	<link rel="stylesheet" type="text/css" href="/theme/css/code.css"/>
</head>

<body>
	<div id="topBox">
		<h1 class="siteTitle"><a href="/index.html">Vibhav Gaur</a></h1>
		<div id="canvasDiv">
			<!-- This div is to make the canvas clickable -->
			<span id="canvasContainer" title="Click to learn more">
			</span>
		</div>
	</div>

	<ul class="NavBar">
			<li><a href="/pages/blog.html">Blog</a></li>
			<li><a href="/pages/coursework.html">Coursework</a></li>
			<li><a href="/pages/projects.html">Projects</a></li>
	</ul>

		<div class="content">
	<h2>Least-Squares&nbsp;fitting</h2>
	<label>Posted on <strong>June 26, 2021</strong></label>
		
	<p>The method of Least-Squares is an intuitive mathematical approach to fitting a curve (or surface, or hypersurface) to a set of data. 
For a 1-dimensional data set, each data point has an $x$ coordinate and a $y$ coordinate &#8212; since we need one additional dimension to visualize the output, but this can be expanded to as many dimensions as needed.
So, an example data set can be
$$ (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)&nbsp;$$</p>
<h3>Linear least&nbsp;squares</h3>
<p>If we wanted to fit a line through the above data, we would effectively be saying that we want to approximate the data set by the&nbsp;function</p>
<p>$$ y = f(x) = Ax + B&nbsp;$$</p>
<p>where the &#8220;fitting&#8221; process is the process of finding $A$ and $B$ given some constraint &#8212; in this case some kind of error minimization constraint.
This error constraint can be defined in a variety of ways, for&nbsp;example:</p>
<ul>
<li>Average error: $E_{avg}(f) = \frac{1}{n} \sum_{k=1}^n |f(x_k) - y_k|$. This is just averaging the errors between each prediction and corresponding data&nbsp;point.</li>
<li>Root-mean-square error: $E_{rms}(f) = \sqrt{ \frac{1}{n} \sum_{k=1}^n |f(x_k) - y_k|^2}$. Anytime there is a root-mean-square value anywhere, its calculated by performing those operations in the reverse order.
Take the error, square it, calculate its mean, and then square root it &#8212; voila, the <em>root-mean-square</em> value of the error.
This is the most popular constraint on the error used in practice.
For reasons that will be clear shortly ahead, using simply the mean-square error is also popular in practice and will be used for the following&nbsp;discussion.</li>
</ul>
<p>As mentioned previously, &#8220;fitting&#8221; means finding the parameters $A$ and $B$ of the &#8220;curve&#8221; (in this case a line) such that $E_{rms}$ is minimized.
This calls for some calculus, specifically, setting the derivative of $E_{rms}$ to $0$ with respect to the parameters $A$ and&nbsp;$B$.</p>
<p>$$ \frac{\partial{E_{rms}}}{\partial A} = 0 \text{ and } \frac{\partial E_{rms}}{\partial B} = 0&nbsp;$$</p>
<p>Minimizing $E_{rms} = \sqrt{ \frac{1}{n} \sum_{k=1}^n |f(x_k) - y_k|^2}$ is the same as minimizing the square of this quantity, so we&nbsp;write</p>
<p>$$ E_{rms} = \sum_{k=1}^{n} |f(x_k) - y_k|^2 = \sum_{k=1}^{n} (Ax_k + B - y_k)^2&nbsp;$$</p>
<p>To minimize this with respect to $A$ and $B$ we calculated the partial derivatives and set them to&nbsp;$0$:</p>
<p>\begin{align}
\frac{\partial E_{rms}}{\partial A} = \sum_{k=1}^{n} 2(Ax_k + B - y_k) x_k = 0  \
\frac{\partial E_{rms}}{\partial B} = \sum_{k=1}^{n} 2(Ax_k + B - y_k) = 0&nbsp;\end{align}</p>
<p>The above is a linear system of equations in $A$ and&nbsp;$B$:</p>
<p>$$ \begin{bmatrix}
    \sum_{k=1}^{n} x_k^2 <span class="amp">&amp;</span> \sum_{k=1}^{n} x_k \
    \sum_{k=1}^{n} x_k   <span class="amp">&amp;</span> \sum_{k=1}^{n}
   \end{bmatrix} \begin{bmatrix}
            A \
            B
         \end{bmatrix} = \begin{bmatrix}
                    \sum_{k=1}^{n} x_k y_k \
                    \sum_{k=1}^{n} y_k
                \end{bmatrix}&nbsp;$$</p>
<p>This system of linear equations is easily solved by many available software packages like NumPy or <span class="caps">MATLAB</span>.</p>
<p>A different type of curve can be fit just as easily by including the additional parameters in the resultant linear system.
For example, fitting a second degree polynomial would go something like&nbsp;this:</p>
<p>$$ y = f(x) = Ax^2 + Bx + C&nbsp;$$</p>
<p>which has 3 parameters, $A$, $B$, and $C$.
This (quadratic) curve could be fit to the data using the same methods outlined above.
This would result in a linear system of equations generated by setting the partial derivatives to&nbsp;$0$</p>
<p>$$ \frac{\partial E_{rms}}{\partial A} = 0 \text{, } \frac{\partial E_{rms}}{\partial B} = 0 \text{, } \frac{\partial E_{rms}}{\partial C} = 0&nbsp;$$</p>
<h4>Spatial inuition for a least squares&nbsp;fit</h4>
<p>To develop some spatial intuition about least squares fitting, consider the 1-dimensional data case where the data is given&nbsp;by</p>
<p>$$ (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)&nbsp;$$</p>
<p>Let&#8217;s consider what is going when when we minimize the <span class="caps">RMS</span> error for this data set by considering just one data point.
This is best described in the following&nbsp;picture:</p>
<p align="center">
<img src="../images/LeastSquares/LeastSquares_Intuition.png">
</p>

<p>When one minimizes the error (or some form of it) for a given data point $(x_k, y_k)$, marked on the above picture as $|f(x_k) - y_k|$, one is actually parameterizing the line that overall has the least distance (along the <em>y</em>-direction, depending on how the error is defined) from <em>all</em> the data points (since the error is summed for all data points). 
One may define the error as the perpendicular distance of the data points from the fit line, in which case the distance <em>p</em> would be minimized for each data point.
One can take this intuition to higher dimensions where, in the linear fit case, one is fitting a plane (or hyperplane with appropriate dimensionality) to the data set.
The definition of the error will determine which &#8220;distance&#8221; is being minimized.
In nonlinear fitting, the curve being fit generalizes to a hypersurface of appropriate&nbsp;dimensionality.</p>
<h3>Nonlinear least&nbsp;squares</h3>
<p>The system of equations generated by the minimization problem above is not always a linear system.
If the function (curve) used to fit the data is a nonlinear function of the paramters, then it is likely going to result in a nonlinear set of equations.
If you don&#8217;t believe me, then try to get the system of equations formed when one tries to fit the function $y = f(x) = e^{Ax}$ to the data.
Such a system cannot be easily solved using analytical methods which we have been using above.
In fact, a solution may not even exist.
Or many solutions may exist.
Such a system can be solved using an iterative numerical procedure like <a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton-Raphson</a>.
In general, fitting a function of $M (M &lt; n)$ parameters $y = f(x, C_1, C_2, \dots, C_M)$ to the data will generate a system of nonlinear equations which will have to be solved&nbsp;numerically.</p>
<p><em>Read my post on <a href="./lsq-visualized-overdetermined.html">least-squares applied to an overdetermined system</a> for a better visual explanation of the above in&nbsp;action.</em></p>
<h3>References:</h3>
<p><em><a href="https://www.amazon.com/Data-Driven-Modeling-Scientific-Computation-Methods/dp/0199660344">Data-Driven Modeling and Scientific Computation: Methods for Complex Systems and Big Data</a></em> by <a href="http://faculty.washington.edu/kutz/">Nathan&nbsp;Kutz</a></p>
<p>For any questions regarding this post, reach out to me using one of the methods&nbsp;below.</p>
		</div>

		<footer>
			<ul class="footer">
				<li><a href="https://github.com/vibhavgaur">GitHub</a></li>
				<li><a href="mailto:vibhavgaur@me.com">Email</a></li>
				<li><a href="https://twitter.com/VibhavGaur">Twitter</a></li>
				<li><a href="https://www.linkedin.com/in/vibhav-gaur-775815137/">LinkedIn</a></li>
			</ul>
		</footer>
</body>

</html>